{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing some interesting plots\n",
    "\n",
    "Thought :\n",
    "\n",
    "do : create synthetic datasets where 50% of the data is synthetic, and show how long (duration), amplitude (how heigh), or frequency (power) needs to be added for the abnormality to be detectable for a simple CNN.\n",
    "\n",
    "\n",
    "1st: set duration 100 ms, amplitude 1 --> Add frequency 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600 , 1700, 1800, 1900, 2000\n",
    "\n",
    "2nd: Set frequency = 400 Hz, duration = 100, and do: amplitude 0.5 - 10 with 0.5 step\n",
    "\n",
    "3rd: Set frequency = 400 Hz, amplitude = 1 --> add duration 50 ms to 1 s by 50 ms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.insert(1, module_path + '/src')\n",
    "\n",
    "import utility\n",
    "\n",
    "import librosa\n",
    "import sktime\n",
    "from sktime.utils.data_io import load_from_tsfile_to_dataframe\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import librosa\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from scipy.stats import skew \n",
    "import random\n",
    "\n",
    "sys.path.insert(1, '/home/andrine/Desktop/tqwt_tools')\n",
    "from tqwt_tools import DualQDecomposition\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import stft\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.layers import Conv1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from hypopt import GridSearch\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(1, module_path + '/src/')\n",
    "\n",
    "def znorm(timeseries):\n",
    "    mean = np.mean(timeseries)\n",
    "    sd = np.std(timeseries)\n",
    "    return (timeseries - mean)/sd\n",
    "\n",
    "target_rate = 44100\n",
    "ds_target_rate = 5000\n",
    "\n",
    "import os\n",
    "names = []\n",
    "#files_path = '/home/andrine/Desktop/dataTromso/Hasse413/'\n",
    "#files_path = '/home/andrine/Desktop/dataTromsoFiltered/allFilteres/'\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(module_path + '/data/tromsoSummary.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "\n",
    "new_dataset = data[data['label'] == 'normal'].copy(deep = True).iloc[:4000]\n",
    "del data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "new_dataset.sort_values(by=['nr'], inplace=True)\n",
    "\n",
    "id_unique = new_dataset['nr'].unique()\n",
    "\n",
    "id_train , id_test = train_test_split(id_unique, test_size=0.2, random_state=42)\n",
    "id_train , id_val = train_test_split(id_unique, test_size=0.25, random_state=42)\n",
    "\n",
    "new_dataset.reset_index(drop = False)\n",
    "\n",
    "\n",
    "def get_indices(id_list, data):\n",
    "    indices = np.array([])\n",
    "    for i in id_list:\n",
    "        idx = np.where(data['nr'] == i)[0]\n",
    "        indices = np.append([list(idx)] , [indices])\n",
    "        \n",
    "    return indices\n",
    "\n",
    "indices_train = get_indices(id_train, new_dataset).astype(int)\n",
    "indices_test = get_indices(id_test, new_dataset).astype(int)\n",
    "indices_val = get_indices(id_val, new_dataset).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(ts, fixed_len):\n",
    "    df_new = np.zeros(fixed_len)\n",
    "    len_ts = len(ts)\n",
    "    pad = (fixed_len - len_ts)//2\n",
    "    df_new[pad:pad + len_ts] = ts\n",
    "    return df_new\n",
    "\n",
    "\n",
    "lowcut = 120\n",
    "highcut = 2400\n",
    "FRAME_RATE = 8000\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def bandpass_filter(buffer):\n",
    "    return butter_bandpass_filter(buffer, lowcut, highcut, FRAME_RATE, order=12)\n",
    "\n",
    "def get_sub_audios_no_overlap(audio, frac):\n",
    "    base = len(audio)//frac\n",
    "    sub_audio = []\n",
    "    for n in range(frac):\n",
    "        #print(n*base)\n",
    "        if (n+1)*base < len(audio):\n",
    "            sub = audio[n*base : (n+1)*base]\n",
    "            sub_audio.append(sub)\n",
    "            #print(len(sub))\n",
    "        else:\n",
    "            sub = audio[n*base :]\n",
    "            #print(len(sub))\n",
    "            diff = len(sub) - len(sub_audio[-1])\n",
    "            if diff != 0:\n",
    "                print(diff)\n",
    "                sub = audio[n*base - diff :]\n",
    "            sub_audio.append(sub)\n",
    "            #print(len(sub))\n",
    "            #print(diff)\n",
    "    \n",
    "\n",
    "    return sub_audio\n",
    "\n",
    "def make_synthetic(audio, sr, dur , ampl,  freq ):\n",
    "    random_offset = random.random()\n",
    "    test = ampl*librosa.tone(freq, sr=sr, length=dur*sr, phi = random_offset)\n",
    "    random_index = random.randint(0, len(audio) - len(test))\n",
    "    new_audio = audio.copy()\n",
    "    new_audio[random_index : random_index + len(test)] = new_audio[random_index : random_index + len(test)]  + test  \n",
    "    return new_audio\n",
    "\n",
    "def get_features(data, sr):\n",
    "    sr = sr\n",
    "    n_mfcc = 30\n",
    "    ft1 = librosa.feature.mfcc(data, sr = sr, n_mfcc=n_mfcc)\n",
    "    ft2 = librosa.feature.zero_crossing_rate(data)[0]\n",
    "    ft3 = librosa.feature.spectral_rolloff(data)[0]\n",
    "    ft4 = librosa.feature.spectral_centroid(data)[0]\n",
    "    ft5 = librosa.feature.spectral_contrast(data)[0]\n",
    "    ft6 = librosa.feature.spectral_bandwidth(data)[0]\n",
    "\n",
    "    ### Get HOS and simple features \n",
    "    ft0_trunc = np.hstack((np.mean(data) , np.std(data), skew(data), np.max(data), np.median(data), np.min(data), utility.get_energy(data), utility.get_entropy(data)))\n",
    "\n",
    "    ### MFCC features\n",
    "    ft1_trunc = np.hstack((np.mean(ft1, axis=1), np.std(ft1, axis=1), skew(ft1, axis = 1), np.max(ft1, axis = 1), np.median(ft1, axis = 1), np.min(ft1, axis = 1)))\n",
    "\n",
    "    ### Spectral Features \n",
    "    ft2_trunc = np.hstack((np.mean(ft2), np.std(ft2), skew(ft2), np.max(ft2), np.median(ft2), np.min(ft2)))\n",
    "    ft3_trunc = np.hstack((np.mean(ft3), np.std(ft3), skew(ft3), np.max(ft3), np.median(ft3), np.min(ft3)))\n",
    "    ft4_trunc = np.hstack((np.mean(ft4), np.std(ft4), skew(ft4), np.max(ft4), np.median(ft4), np.min(ft4)))\n",
    "    ft5_trunc = np.hstack((np.mean(ft5), np.std(ft5), skew(ft5), np.max(ft5), np.median(ft5), np.min(ft5)))\n",
    "    ft6_trunc = np.hstack((np.mean(ft6), np.std(ft6), skew(ft6), np.max(ft6), np.median(ft6), np.max(ft6)))\n",
    "    return pd.Series(np.hstack((ft0_trunc , ft1_trunc, ft2_trunc, ft3_trunc, ft4_trunc, ft5_trunc, ft6_trunc)))\n",
    "\n",
    "\n",
    "def get_data_array(data, dur = 0.1, ampl = 1,  freq = 400 ):\n",
    "    error_in_data = {}\n",
    "    count = 0\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    for row in data.iterrows():\n",
    "\n",
    "        audio_file = row[1]['id']\n",
    "        label = row[1]['label']\n",
    "\n",
    "        try:\n",
    "            sr, audio = utility.read_wav_file(audio_file, target_rate)\n",
    "\n",
    "        except EOFError as error:\n",
    "            error_in_data[audio_file] = 'EOFError'\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            audio = utility.denoise_audio(audio)\n",
    "            audio, sr = utility.downsample(audio, sr, 8000), 8000\n",
    "\n",
    "            audio = np.apply_along_axis(bandpass_filter, 0, audio).astype('float64')\n",
    "            audio = znorm(audio)\n",
    "            '''\n",
    "            if (count%2 == 0):\n",
    "                audio = make_synthetic(audio, sr, dur, ampl,  freq )\n",
    "                label = 'synthetic'\n",
    "            else:\n",
    "                label = 'normal'\n",
    "            \n",
    "            #audio = zero_pad(audio, 75000)\n",
    "            features = get_features(audio, sr)\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            \n",
    "            \n",
    "            count = count + 1\n",
    "            \n",
    "            '''\n",
    "            sub_audio = get_sub_audios_no_overlap(audio, 3) # Set default 5 second window\n",
    "            \n",
    "            \n",
    "            for sub in sub_audio:  \n",
    "                if (count%2 == 0):\n",
    "                    sub = make_synthetic(sub, sr, dur, ampl,  freq )\n",
    "                    label = 'synthetic'\n",
    "                else:\n",
    "                    label = 'normal'\n",
    "                    \n",
    "                count = count + 1\n",
    "                sub = zero_pad(sub, 25000)\n",
    "                #sub = get_features(sub, sr)\n",
    "                \n",
    "                X.append(sub)\n",
    "                y.append(label)\n",
    "            \n",
    "\n",
    "        except ValueError as error:\n",
    "            error_in_data[audio_file] = 'ValueError'\n",
    "            continue\n",
    "    return (np.array(X), pd.Series(y))\n",
    "\n",
    "\n",
    "def get_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv1D(filters=50, kernel_size=6, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=50))\n",
    "    model.add(Conv1D(filters=50, kernel_size=6, activation='relu' ,  padding=\"same\"))\n",
    "    #model.add(MaxPooling1D(pool_size=100))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1500, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    #model.add(Dense(500, activation='relu'))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    optimizer = keras.optimizers.Nadam(lr=0.001)\n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def get_CNN_ready_dataset(y, le):\n",
    "    num_classes = len(le.classes_)\n",
    "    y =  le.transform(y)\n",
    "    y = np_utils.to_categorical(y, num_classes = num_classes)\n",
    "    return y\n",
    "\n",
    "def get_random_forest_grid():\n",
    "    parameters = {\n",
    "    'n_estimators'      : [300,400,500],\n",
    "    'max_depth'         : [8, 10, 12]}\n",
    "    return GridSearch(model = RandomForestClassifier(criterion='entropy', random_state=42),\n",
    "                      param_grid = parameters)\n",
    "\n",
    "\n",
    "def get_preproject_acc_f1(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    #opt = get_random_forest_grid()\n",
    "    #%time opt.fit(X_train, y_train, X_val, y_val,scoring = 'f1')\n",
    "    clf = RandomForestClassifier(criterion='entropy', max_depth=8, n_estimators=500,random_state=42)\n",
    "    #X, y = np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val])\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = classification_report(y_test, y_pred, output_dict = True)['accuracy']\n",
    "    f1 = classification_report(y_test, y_pred, output_dict = True)['macro avg']['f1-score']\n",
    "    return acc, f1, y_pred, y_test.to_numpy()\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency increase plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrine/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:146: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0edc58b03a95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "indices = {\n",
    "    'train' : indices_train,\n",
    "    'test' : indices_test,\n",
    "    'val' : indices_val\n",
    "    \n",
    "}\n",
    "\n",
    "result_datasets = {\n",
    "    'train' : ([], []),\n",
    "    'test' : ([], []),\n",
    "    'val' : ([], [])\n",
    "    \n",
    "}\n",
    "\n",
    "freqs = np.linspace(100, 2000, 20).astype(int)\n",
    "ampls = np.linspace(0.5, 10, 20)\n",
    "durs = np.linspace(0.01, 0.2, 20)\n",
    "\n",
    "dur = 0.1\n",
    "ampl = 1\n",
    "freq = 400\n",
    "acc_freqs = []\n",
    "f1_freqs = []\n",
    "\n",
    "for freq in freqs:\n",
    "    #print(freq)\n",
    "    # Get the datasets\n",
    "    for name, i in indices.items():\n",
    "        result_datasets[name] = get_data_array(new_dataset.iloc[i], dur, ampl, freq )\n",
    "        \n",
    "    X_train, y_train = result_datasets['train']\n",
    "    X_test, y_test = result_datasets['test']\n",
    "    X_val, y_val = result_datasets['val']\n",
    "    \n",
    "    classes = y_train.unique()\n",
    "    \n",
    "    # Preparing the data for the CNN\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    le.fit(y_train)\n",
    "    num_classes = len(le.classes_)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state = 42)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0],X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],X_val.shape[1], 1)\n",
    "\n",
    "\n",
    "    y_train = get_CNN_ready_dataset(y_train, le)\n",
    "    y_test = get_CNN_ready_dataset(y_test, le)\n",
    "    y_val = get_CNN_ready_dataset(y_val, le)\n",
    "    \n",
    "    # Training a simple CNN\n",
    "    \n",
    "    batch_size = 100\n",
    "    epochs = 20\n",
    "    input_shape=(X_train.shape[1], 1)\n",
    "\n",
    "    model = get_model(input_shape, num_classes)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs = epochs,\n",
    "              batch_size = batch_size,\n",
    "              verbose = 1,\n",
    "             validation_data = (X_val, y_val))\n",
    "    \n",
    "    \n",
    "    result = model.predict(X_test)\n",
    "    y_pred = np.argmax(result, axis = 1)\n",
    "    y_true = np.argmax(y_test, axis = 1)\n",
    "    \n",
    "    '''acc, f1, y_pred, y_true = get_preproject_acc_f1(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    f1_freqs.append(f1)\n",
    "    acc_freqs.append(acc)'''\n",
    "    \n",
    "    f1_freqs.append(f1_score(y_true, y_pred))\n",
    "    acc_freqs.append(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    f, ax = utility.plot_cm(y_true, y_pred, module_path = module_path, class_names = classes, color_index = 6)\n",
    "    plt.show()\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/acc_freqs_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_freqs, f)\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/f1_freqs_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_freqs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amplitude increase plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {\n",
    "    'train' : indices_train,\n",
    "    'test' : indices_test,\n",
    "    'val' : indices_val\n",
    "    \n",
    "}\n",
    "\n",
    "result_datasets = {\n",
    "    'train' : ([], []),\n",
    "    'test' : ([], []),\n",
    "    'val' : ([], [])\n",
    "    \n",
    "}\n",
    "\n",
    "freqs = np.linspace(100, 2000, 20).astype(int)\n",
    "ampls = np.linspace(0.5, 10, 20)\n",
    "durs = np.linspace(0.01, 0.2, 20)\n",
    "\n",
    "dur = 0.1\n",
    "ampl = 1\n",
    "freq = 400\n",
    "acc_ampl = []\n",
    "f1_ampl = []\n",
    "\n",
    "for ampl in ampls:\n",
    "    for name, i in indices.items():\n",
    "        result_datasets[name] = get_data_array(new_dataset.iloc[i], dur, ampl, freq )\n",
    "        \n",
    "    X_train, y_train = result_datasets['train']\n",
    "    X_test, y_test = result_datasets['test']\n",
    "    X_val, y_val = result_datasets['val']\n",
    "    \n",
    "    classes = y_train.unique()\n",
    "    \n",
    "    # Preparing the data for the CNN\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    le.fit(y_train)\n",
    "    num_classes = len(le.classes_)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state = 42)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0],X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],X_val.shape[1], 1)\n",
    "\n",
    "\n",
    "    y_train = get_CNN_ready_dataset(y_train, le)\n",
    "    y_test = get_CNN_ready_dataset(y_test, le)\n",
    "    y_val = get_CNN_ready_dataset(y_val, le)\n",
    "    \n",
    "    # Training a simple CNN\n",
    "    \n",
    "    batch_size = 100\n",
    "    epochs = 20\n",
    "    input_shape=(X_train.shape[1], 1)\n",
    "\n",
    "    model = get_model(input_shape, num_classes)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs = epochs,\n",
    "              batch_size = batch_size,\n",
    "              verbose = 0,\n",
    "             validation_data = (X_val, y_val))\n",
    "    \n",
    "    \n",
    "    result = model.predict(X_test)\n",
    "    y_pred = np.argmax(result, axis = 1)\n",
    "    y_true = np.argmax(y_test, axis = 1)\n",
    "    \n",
    "    #acc, f1, y_pred, y_true = get_preproject_acc_f1(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    #f1_ampl.append(f1)\n",
    "    #acc_ampl.append(acc)\n",
    "    \n",
    "    f1_ampl.append(f1_score(y_true, y_pred))\n",
    "    acc_ampl.append(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    f, ax = utility.plot_cm(y_true, y_pred, module_path = module_path, class_names = le.classes_, color_index = 6)\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/acc_ampls_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_ampl, f)\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/f1_ampls_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_ampl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration increase plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {\n",
    "    'train' : indices_train,\n",
    "    'test' : indices_test,\n",
    "    'val' : indices_val\n",
    "    \n",
    "}\n",
    "\n",
    "result_datasets = {\n",
    "    'train' : ([], []),\n",
    "    'test' : ([], []),\n",
    "    'val' : ([], [])\n",
    "    \n",
    "}\n",
    "\n",
    "freqs = np.linspace(100, 2000, 20).astype(int)\n",
    "ampls = np.linspace(0.5, 10, 20)\n",
    "durs = np.linspace(0.01, 0.2, 20)\n",
    "\n",
    "dur = 0.1\n",
    "ampl = 1\n",
    "freq = 400\n",
    "acc_dur = []\n",
    "f1_dur = []\n",
    "\n",
    "for dur in durs:\n",
    "    for name, i in indices.items():\n",
    "        result_datasets[name] = get_data_array(new_dataset.iloc[i], dur, ampl, freq )\n",
    "        \n",
    "    X_train, y_train = result_datasets['train']\n",
    "    X_test, y_test = result_datasets['test']\n",
    "    X_val, y_val = result_datasets['val']\n",
    "    \n",
    "    classes = y_train.unique()\n",
    "    \n",
    "    # Preparing the data for the CNN\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    le.fit(y_train)\n",
    "    num_classes = len(le.classes_)\n",
    "    X_train, y_train = shuffle(X_train, y_train, random_state = 42)\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0],X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0],X_test.shape[1], 1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],X_val.shape[1], 1)\n",
    "\n",
    "\n",
    "    y_train = get_CNN_ready_dataset(y_train, le)\n",
    "    y_test = get_CNN_ready_dataset(y_test, le)\n",
    "    y_val = get_CNN_ready_dataset(y_val, le)\n",
    "    \n",
    "    # Training a simple CNN\n",
    "    \n",
    "    batch_size = 100\n",
    "    epochs = 20\n",
    "    input_shape=(X_train.shape[1], 1)\n",
    "\n",
    "    model = get_model(input_shape, num_classes)\n",
    "\n",
    "    model.fit(X_train, y_train, epochs = epochs,\n",
    "              batch_size = batch_size,\n",
    "              verbose = 0,\n",
    "             validation_data = (X_val, y_val))\n",
    "    \n",
    "    \n",
    "    result = model.predict(X_test)\n",
    "    y_pred = np.argmax(result, axis = 1)\n",
    "    y_true = np.argmax(y_test, axis = 1)\n",
    "    \n",
    "    f1_dur.append(f1_score(y_true, y_pred))\n",
    "    acc_dur.append(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    f, ax = utility.plot_cm(y_true, y_pred, module_path = module_path, class_names = le.classes_, color_index = 6)\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/acc_durs_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_dur, f)\n",
    "    \n",
    "with open(module_path + '/src/visualization/synthetic_increase_acc_f1/f1_durs_5_newCNN.pkl', 'wb') as f:\n",
    "    pickle.dump(f1_dur, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
