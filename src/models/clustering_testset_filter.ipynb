{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "declared-browse",
   "metadata": {},
   "source": [
    "# Using clusters to filter the test set\n",
    "\n",
    "Idea A:\n",
    "\n",
    "As you know we have been experimenting with the double classifier for uncertainty determination.\n",
    "\n",
    "In the original pitch for the masters thesis I suggested using unsupervised learning for this so I suggest we also look into this and it would give you another uncertainity method for the masters.\n",
    "\n",
    "The method would be something like this.\n",
    "\n",
    "1. Train the supervised algorithm on the training dataset.\n",
    "\n",
    "2. Use unsupervised learning on the training set and identify clusters of data.\n",
    "\n",
    "3. Identify the same clusters in the validation dataset and test each cluster individually. Take note of the difference between best and worst performing clusters.\n",
    "\n",
    "4. Repeat steps 2 and 3 until you find the unsupervised algorithm and hyperparameters with the largest difference between best and worst performing clusters.\n",
    "\n",
    "5. Identify the same clusters in the test dataset using the unsupervised algorithm and hyperparameters found.\n",
    "\n",
    "6. Test the supervised algorithm in the test dataset but exclude the worst performing cluster. This should increase accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pursuant-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.insert(1, module_path + '/src')\n",
    "\n",
    "from sktime.utils.data_io import load_from_tsfile_to_dataframe\n",
    "from sktime.utils.data_processing import from_nested_to_2d_array\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "touched-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_from_tsfile_to_dataframe(module_path + '/features/extracted_features_ts_files/lungsound_preproject_TRAIN.ts')\n",
    "\n",
    "X_test, y_test = load_from_tsfile_to_dataframe(module_path + '/features/extracted_features_ts_files/lungsound_preproject_TEST.ts')\n",
    "\n",
    "X_train = from_nested_to_2d_array(X_train)\n",
    "X_test = from_nested_to_2d_array(X_test)\n",
    "\n",
    "y_test = y_test.astype(int)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "X_train.columns = np.arange(len(X_train.columns))\n",
    "X_test.columns = np.arange(len(X_test.columns))\n",
    "\n",
    "\n",
    "X_train = X_train.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-buddy",
   "metadata": {},
   "source": [
    "## PCA and ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pacific-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICA and PCA (first 2 components)\n",
    "from sklearn.decomposition import PCA, FastICA # Principal Component Analysis module\n",
    "ica = FastICA(n_components=10)\n",
    "ica.fit(X_train)\n",
    "ic = ica.transform(X_train)\n",
    "ic_test = ica.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "pc = pca.transform(X_train)\n",
    "pc_test = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-gauge",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tender-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 10\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(X_train.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "# let's create and compile the autoencoder\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinguished-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "23/23 - 0s - loss: 0.6477 - val_loss: 0.6384\n",
      "Epoch 2/300\n",
      "23/23 - 0s - loss: 0.6463 - val_loss: 0.6373\n",
      "Epoch 3/300\n",
      "23/23 - 0s - loss: 0.6452 - val_loss: 0.6362\n",
      "Epoch 4/300\n",
      "23/23 - 0s - loss: 0.6441 - val_loss: 0.6352\n",
      "Epoch 5/300\n",
      "23/23 - 0s - loss: 0.6430 - val_loss: 0.6342\n",
      "Epoch 6/300\n",
      "23/23 - 0s - loss: 0.6420 - val_loss: 0.6333\n",
      "Epoch 7/300\n",
      "23/23 - 0s - loss: 0.6411 - val_loss: 0.6324\n",
      "Epoch 8/300\n",
      "23/23 - 0s - loss: 0.6402 - val_loss: 0.6316\n",
      "Epoch 9/300\n",
      "23/23 - 0s - loss: 0.6393 - val_loss: 0.6307\n",
      "Epoch 10/300\n",
      "23/23 - 0s - loss: 0.6384 - val_loss: 0.6298\n",
      "Epoch 11/300\n",
      "23/23 - 0s - loss: 0.6375 - val_loss: 0.6289\n",
      "Epoch 12/300\n",
      "23/23 - 0s - loss: 0.6366 - val_loss: 0.6281\n",
      "Epoch 13/300\n",
      "23/23 - 0s - loss: 0.6358 - val_loss: 0.6272\n",
      "Epoch 14/300\n",
      "23/23 - 0s - loss: 0.6349 - val_loss: 0.6263\n",
      "Epoch 15/300\n",
      "23/23 - 0s - loss: 0.6340 - val_loss: 0.6254\n",
      "Epoch 16/300\n",
      "23/23 - 0s - loss: 0.6332 - val_loss: 0.6245\n",
      "Epoch 17/300\n",
      "23/23 - 0s - loss: 0.6323 - val_loss: 0.6236\n",
      "Epoch 18/300\n",
      "23/23 - 0s - loss: 0.6314 - val_loss: 0.6227\n",
      "Epoch 19/300\n",
      "23/23 - 0s - loss: 0.6306 - val_loss: 0.6218\n",
      "Epoch 20/300\n",
      "23/23 - 0s - loss: 0.6297 - val_loss: 0.6209\n",
      "Epoch 21/300\n",
      "23/23 - 0s - loss: 0.6289 - val_loss: 0.6199\n",
      "Epoch 22/300\n",
      "23/23 - 0s - loss: 0.6280 - val_loss: 0.6190\n",
      "Epoch 23/300\n",
      "23/23 - 0s - loss: 0.6272 - val_loss: 0.6181\n",
      "Epoch 24/300\n",
      "23/23 - 0s - loss: 0.6264 - val_loss: 0.6172\n",
      "Epoch 25/300\n",
      "23/23 - 0s - loss: 0.6255 - val_loss: 0.6162\n",
      "Epoch 26/300\n",
      "23/23 - 0s - loss: 0.6247 - val_loss: 0.6153\n",
      "Epoch 27/300\n",
      "23/23 - 0s - loss: 0.6239 - val_loss: 0.6144\n",
      "Epoch 28/300\n",
      "23/23 - 0s - loss: 0.6231 - val_loss: 0.6135\n",
      "Epoch 29/300\n",
      "23/23 - 0s - loss: 0.6223 - val_loss: 0.6126\n",
      "Epoch 30/300\n",
      "23/23 - 0s - loss: 0.6215 - val_loss: 0.6117\n",
      "Epoch 31/300\n",
      "23/23 - 0s - loss: 0.6208 - val_loss: 0.6108\n",
      "Epoch 32/300\n",
      "23/23 - 0s - loss: 0.6200 - val_loss: 0.6099\n",
      "Epoch 33/300\n",
      "23/23 - 0s - loss: 0.6193 - val_loss: 0.6090\n",
      "Epoch 34/300\n",
      "23/23 - 0s - loss: 0.6185 - val_loss: 0.6082\n",
      "Epoch 35/300\n",
      "23/23 - 0s - loss: 0.6178 - val_loss: 0.6073\n",
      "Epoch 36/300\n",
      "23/23 - 0s - loss: 0.6171 - val_loss: 0.6065\n",
      "Epoch 37/300\n",
      "23/23 - 0s - loss: 0.6164 - val_loss: 0.6057\n",
      "Epoch 38/300\n",
      "23/23 - 0s - loss: 0.6157 - val_loss: 0.6049\n",
      "Epoch 39/300\n",
      "23/23 - 0s - loss: 0.6150 - val_loss: 0.6041\n",
      "Epoch 40/300\n",
      "23/23 - 0s - loss: 0.6143 - val_loss: 0.6033\n",
      "Epoch 41/300\n",
      "23/23 - 0s - loss: 0.6137 - val_loss: 0.6026\n",
      "Epoch 42/300\n",
      "23/23 - 0s - loss: 0.6130 - val_loss: 0.6018\n",
      "Epoch 43/300\n",
      "23/23 - 0s - loss: 0.6124 - val_loss: 0.6011\n",
      "Epoch 44/300\n",
      "23/23 - 0s - loss: 0.6117 - val_loss: 0.6004\n",
      "Epoch 45/300\n",
      "23/23 - 0s - loss: 0.6111 - val_loss: 0.5997\n",
      "Epoch 46/300\n",
      "23/23 - 0s - loss: 0.6105 - val_loss: 0.5990\n",
      "Epoch 47/300\n",
      "23/23 - 0s - loss: 0.6099 - val_loss: 0.5983\n",
      "Epoch 48/300\n",
      "23/23 - 0s - loss: 0.6093 - val_loss: 0.5977\n",
      "Epoch 49/300\n",
      "23/23 - 0s - loss: 0.6087 - val_loss: 0.5971\n",
      "Epoch 50/300\n",
      "23/23 - 0s - loss: 0.6082 - val_loss: 0.5964\n",
      "Epoch 51/300\n",
      "23/23 - 0s - loss: 0.6076 - val_loss: 0.5958\n",
      "Epoch 52/300\n",
      "23/23 - 0s - loss: 0.6071 - val_loss: 0.5952\n",
      "Epoch 53/300\n",
      "23/23 - 0s - loss: 0.6065 - val_loss: 0.5946\n",
      "Epoch 54/300\n",
      "23/23 - 0s - loss: 0.6060 - val_loss: 0.5940\n",
      "Epoch 55/300\n",
      "23/23 - 0s - loss: 0.6054 - val_loss: 0.5935\n",
      "Epoch 56/300\n",
      "23/23 - 0s - loss: 0.6049 - val_loss: 0.5929\n",
      "Epoch 57/300\n",
      "23/23 - 0s - loss: 0.6044 - val_loss: 0.5924\n",
      "Epoch 58/300\n",
      "23/23 - 0s - loss: 0.6039 - val_loss: 0.5919\n",
      "Epoch 59/300\n",
      "23/23 - 0s - loss: 0.6034 - val_loss: 0.5914\n",
      "Epoch 60/300\n",
      "23/23 - 0s - loss: 0.6029 - val_loss: 0.5908\n",
      "Epoch 61/300\n",
      "23/23 - 0s - loss: 0.6024 - val_loss: 0.5904\n",
      "Epoch 62/300\n",
      "23/23 - 0s - loss: 0.6019 - val_loss: 0.5899\n",
      "Epoch 63/300\n",
      "23/23 - 0s - loss: 0.6014 - val_loss: 0.5894\n",
      "Epoch 64/300\n",
      "23/23 - 0s - loss: 0.6010 - val_loss: 0.5889\n",
      "Epoch 65/300\n",
      "23/23 - 0s - loss: 0.6005 - val_loss: 0.5885\n",
      "Epoch 66/300\n",
      "23/23 - 0s - loss: 0.6000 - val_loss: 0.5880\n",
      "Epoch 67/300\n",
      "23/23 - 0s - loss: 0.5996 - val_loss: 0.5876\n",
      "Epoch 68/300\n",
      "23/23 - 0s - loss: 0.5991 - val_loss: 0.5872\n",
      "Epoch 69/300\n",
      "23/23 - 0s - loss: 0.5987 - val_loss: 0.5868\n",
      "Epoch 70/300\n",
      "23/23 - 0s - loss: 0.5982 - val_loss: 0.5864\n",
      "Epoch 71/300\n",
      "23/23 - 0s - loss: 0.5978 - val_loss: 0.5860\n",
      "Epoch 72/300\n",
      "23/23 - 0s - loss: 0.5973 - val_loss: 0.5856\n",
      "Epoch 73/300\n",
      "23/23 - 0s - loss: 0.5969 - val_loss: 0.5852\n",
      "Epoch 74/300\n",
      "23/23 - 0s - loss: 0.5965 - val_loss: 0.5849\n",
      "Epoch 75/300\n",
      "23/23 - 0s - loss: 0.5961 - val_loss: 0.5845\n",
      "Epoch 76/300\n",
      "23/23 - 0s - loss: 0.5956 - val_loss: 0.5841\n",
      "Epoch 77/300\n",
      "23/23 - 0s - loss: 0.5952 - val_loss: 0.5838\n",
      "Epoch 78/300\n",
      "23/23 - 0s - loss: 0.5948 - val_loss: 0.5835\n",
      "Epoch 79/300\n",
      "23/23 - 0s - loss: 0.5944 - val_loss: 0.5831\n",
      "Epoch 80/300\n",
      "23/23 - 0s - loss: 0.5940 - val_loss: 0.5828\n",
      "Epoch 81/300\n",
      "23/23 - 0s - loss: 0.5936 - val_loss: 0.5825\n",
      "Epoch 82/300\n",
      "23/23 - 0s - loss: 0.5932 - val_loss: 0.5822\n",
      "Epoch 83/300\n",
      "23/23 - 0s - loss: 0.5928 - val_loss: 0.5819\n",
      "Epoch 84/300\n",
      "23/23 - 0s - loss: 0.5924 - val_loss: 0.5816\n",
      "Epoch 85/300\n",
      "23/23 - 0s - loss: 0.5920 - val_loss: 0.5813\n",
      "Epoch 86/300\n",
      "23/23 - 0s - loss: 0.5916 - val_loss: 0.5811\n",
      "Epoch 87/300\n",
      "23/23 - 0s - loss: 0.5912 - val_loss: 0.5808\n",
      "Epoch 88/300\n",
      "23/23 - 0s - loss: 0.5908 - val_loss: 0.5805\n",
      "Epoch 89/300\n",
      "23/23 - 0s - loss: 0.5904 - val_loss: 0.5803\n",
      "Epoch 90/300\n",
      "23/23 - 0s - loss: 0.5900 - val_loss: 0.5800\n",
      "Epoch 91/300\n",
      "23/23 - 0s - loss: 0.5896 - val_loss: 0.5798\n",
      "Epoch 92/300\n",
      "23/23 - 0s - loss: 0.5892 - val_loss: 0.5795\n",
      "Epoch 93/300\n",
      "23/23 - 0s - loss: 0.5889 - val_loss: 0.5793\n",
      "Epoch 94/300\n",
      "23/23 - 0s - loss: 0.5885 - val_loss: 0.5791\n",
      "Epoch 95/300\n",
      "23/23 - 0s - loss: 0.5881 - val_loss: 0.5788\n",
      "Epoch 96/300\n",
      "23/23 - 0s - loss: 0.5877 - val_loss: 0.5786\n",
      "Epoch 97/300\n",
      "23/23 - 0s - loss: 0.5874 - val_loss: 0.5784\n",
      "Epoch 98/300\n",
      "23/23 - 0s - loss: 0.5870 - val_loss: 0.5782\n",
      "Epoch 99/300\n",
      "23/23 - 0s - loss: 0.5866 - val_loss: 0.5780\n",
      "Epoch 100/300\n",
      "23/23 - 0s - loss: 0.5862 - val_loss: 0.5778\n",
      "Epoch 101/300\n",
      "23/23 - 0s - loss: 0.5859 - val_loss: 0.5776\n",
      "Epoch 102/300\n",
      "23/23 - 0s - loss: 0.5855 - val_loss: 0.5774\n",
      "Epoch 103/300\n",
      "23/23 - 0s - loss: 0.5851 - val_loss: 0.5772\n",
      "Epoch 104/300\n",
      "23/23 - 0s - loss: 0.5848 - val_loss: 0.5770\n",
      "Epoch 105/300\n",
      "23/23 - 0s - loss: 0.5844 - val_loss: 0.5768\n",
      "Epoch 106/300\n",
      "23/23 - 0s - loss: 0.5841 - val_loss: 0.5766\n",
      "Epoch 107/300\n",
      "23/23 - 0s - loss: 0.5837 - val_loss: 0.5765\n",
      "Epoch 108/300\n",
      "23/23 - 0s - loss: 0.5833 - val_loss: 0.5763\n",
      "Epoch 109/300\n",
      "23/23 - 0s - loss: 0.5830 - val_loss: 0.5761\n",
      "Epoch 110/300\n",
      "23/23 - 0s - loss: 0.5826 - val_loss: 0.5759\n",
      "Epoch 111/300\n",
      "23/23 - 0s - loss: 0.5823 - val_loss: 0.5758\n",
      "Epoch 112/300\n",
      "23/23 - 0s - loss: 0.5819 - val_loss: 0.5756\n",
      "Epoch 113/300\n",
      "23/23 - 0s - loss: 0.5816 - val_loss: 0.5754\n",
      "Epoch 114/300\n",
      "23/23 - 0s - loss: 0.5812 - val_loss: 0.5753\n",
      "Epoch 115/300\n",
      "23/23 - 0s - loss: 0.5809 - val_loss: 0.5751\n",
      "Epoch 116/300\n",
      "23/23 - 0s - loss: 0.5805 - val_loss: 0.5749\n",
      "Epoch 117/300\n",
      "23/23 - 0s - loss: 0.5802 - val_loss: 0.5748\n",
      "Epoch 118/300\n",
      "23/23 - 0s - loss: 0.5798 - val_loss: 0.5746\n",
      "Epoch 119/300\n",
      "23/23 - 0s - loss: 0.5795 - val_loss: 0.5745\n",
      "Epoch 120/300\n",
      "23/23 - 0s - loss: 0.5792 - val_loss: 0.5743\n",
      "Epoch 121/300\n",
      "23/23 - 0s - loss: 0.5788 - val_loss: 0.5741\n",
      "Epoch 122/300\n",
      "23/23 - 0s - loss: 0.5785 - val_loss: 0.5740\n",
      "Epoch 123/300\n",
      "23/23 - 0s - loss: 0.5782 - val_loss: 0.5738\n",
      "Epoch 124/300\n",
      "23/23 - 0s - loss: 0.5778 - val_loss: 0.5736\n",
      "Epoch 125/300\n",
      "23/23 - 0s - loss: 0.5775 - val_loss: 0.5735\n",
      "Epoch 126/300\n",
      "23/23 - 0s - loss: 0.5772 - val_loss: 0.5733\n",
      "Epoch 127/300\n",
      "23/23 - 0s - loss: 0.5768 - val_loss: 0.5732\n",
      "Epoch 128/300\n",
      "23/23 - 0s - loss: 0.5765 - val_loss: 0.5730\n",
      "Epoch 129/300\n",
      "23/23 - 0s - loss: 0.5762 - val_loss: 0.5728\n",
      "Epoch 130/300\n",
      "23/23 - 0s - loss: 0.5759 - val_loss: 0.5727\n",
      "Epoch 131/300\n",
      "23/23 - 0s - loss: 0.5756 - val_loss: 0.5725\n",
      "Epoch 132/300\n",
      "23/23 - 0s - loss: 0.5752 - val_loss: 0.5723\n",
      "Epoch 133/300\n",
      "23/23 - 0s - loss: 0.5749 - val_loss: 0.5722\n",
      "Epoch 134/300\n",
      "23/23 - 0s - loss: 0.5746 - val_loss: 0.5720\n",
      "Epoch 135/300\n",
      "23/23 - 0s - loss: 0.5743 - val_loss: 0.5718\n",
      "Epoch 136/300\n",
      "23/23 - 0s - loss: 0.5740 - val_loss: 0.5717\n",
      "Epoch 137/300\n",
      "23/23 - 0s - loss: 0.5737 - val_loss: 0.5715\n",
      "Epoch 138/300\n",
      "23/23 - 0s - loss: 0.5734 - val_loss: 0.5713\n",
      "Epoch 139/300\n",
      "23/23 - 0s - loss: 0.5731 - val_loss: 0.5712\n",
      "Epoch 140/300\n",
      "23/23 - 0s - loss: 0.5728 - val_loss: 0.5710\n",
      "Epoch 141/300\n",
      "23/23 - 0s - loss: 0.5725 - val_loss: 0.5708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/300\n",
      "23/23 - 0s - loss: 0.5723 - val_loss: 0.5707\n",
      "Epoch 143/300\n",
      "23/23 - 0s - loss: 0.5720 - val_loss: 0.5705\n",
      "Epoch 144/300\n",
      "23/23 - 0s - loss: 0.5717 - val_loss: 0.5704\n",
      "Epoch 145/300\n",
      "23/23 - 0s - loss: 0.5714 - val_loss: 0.5702\n",
      "Epoch 146/300\n",
      "23/23 - 0s - loss: 0.5711 - val_loss: 0.5700\n",
      "Epoch 147/300\n",
      "23/23 - 0s - loss: 0.5709 - val_loss: 0.5699\n",
      "Epoch 148/300\n",
      "23/23 - 0s - loss: 0.5706 - val_loss: 0.5697\n",
      "Epoch 149/300\n",
      "23/23 - 0s - loss: 0.5703 - val_loss: 0.5696\n",
      "Epoch 150/300\n",
      "23/23 - 0s - loss: 0.5701 - val_loss: 0.5694\n",
      "Epoch 151/300\n",
      "23/23 - 0s - loss: 0.5698 - val_loss: 0.5693\n",
      "Epoch 152/300\n",
      "23/23 - 0s - loss: 0.5695 - val_loss: 0.5691\n",
      "Epoch 153/300\n",
      "23/23 - 0s - loss: 0.5693 - val_loss: 0.5690\n",
      "Epoch 154/300\n",
      "23/23 - 0s - loss: 0.5690 - val_loss: 0.5688\n",
      "Epoch 155/300\n",
      "23/23 - 0s - loss: 0.5688 - val_loss: 0.5687\n",
      "Epoch 156/300\n",
      "23/23 - 0s - loss: 0.5685 - val_loss: 0.5685\n",
      "Epoch 157/300\n",
      "23/23 - 0s - loss: 0.5683 - val_loss: 0.5684\n",
      "Epoch 158/300\n",
      "23/23 - 0s - loss: 0.5681 - val_loss: 0.5682\n",
      "Epoch 159/300\n",
      "23/23 - 0s - loss: 0.5678 - val_loss: 0.5681\n",
      "Epoch 160/300\n",
      "23/23 - 0s - loss: 0.5676 - val_loss: 0.5680\n",
      "Epoch 161/300\n",
      "23/23 - 0s - loss: 0.5674 - val_loss: 0.5678\n",
      "Epoch 162/300\n",
      "23/23 - 0s - loss: 0.5671 - val_loss: 0.5677\n",
      "Epoch 163/300\n",
      "23/23 - 0s - loss: 0.5669 - val_loss: 0.5676\n",
      "Epoch 164/300\n",
      "23/23 - 0s - loss: 0.5667 - val_loss: 0.5674\n",
      "Epoch 165/300\n",
      "23/23 - 0s - loss: 0.5665 - val_loss: 0.5673\n",
      "Epoch 166/300\n",
      "23/23 - 0s - loss: 0.5663 - val_loss: 0.5672\n",
      "Epoch 167/300\n",
      "23/23 - 0s - loss: 0.5661 - val_loss: 0.5671\n",
      "Epoch 168/300\n",
      "23/23 - 0s - loss: 0.5659 - val_loss: 0.5669\n",
      "Epoch 169/300\n",
      "23/23 - 0s - loss: 0.5657 - val_loss: 0.5668\n",
      "Epoch 170/300\n",
      "23/23 - 0s - loss: 0.5654 - val_loss: 0.5667\n",
      "Epoch 171/300\n",
      "23/23 - 0s - loss: 0.5652 - val_loss: 0.5666\n",
      "Epoch 172/300\n",
      "23/23 - 0s - loss: 0.5651 - val_loss: 0.5665\n",
      "Epoch 173/300\n",
      "23/23 - 0s - loss: 0.5649 - val_loss: 0.5664\n",
      "Epoch 174/300\n",
      "23/23 - 0s - loss: 0.5647 - val_loss: 0.5663\n",
      "Epoch 175/300\n",
      "23/23 - 0s - loss: 0.5645 - val_loss: 0.5662\n",
      "Epoch 176/300\n",
      "23/23 - 0s - loss: 0.5643 - val_loss: 0.5661\n",
      "Epoch 177/300\n",
      "23/23 - 0s - loss: 0.5641 - val_loss: 0.5660\n",
      "Epoch 178/300\n",
      "23/23 - 0s - loss: 0.5639 - val_loss: 0.5659\n",
      "Epoch 179/300\n",
      "23/23 - 0s - loss: 0.5638 - val_loss: 0.5658\n",
      "Epoch 180/300\n",
      "23/23 - 0s - loss: 0.5636 - val_loss: 0.5657\n",
      "Epoch 181/300\n",
      "23/23 - 0s - loss: 0.5634 - val_loss: 0.5656\n",
      "Epoch 182/300\n",
      "23/23 - 0s - loss: 0.5632 - val_loss: 0.5655\n",
      "Epoch 183/300\n",
      "23/23 - 0s - loss: 0.5631 - val_loss: 0.5655\n",
      "Epoch 184/300\n",
      "23/23 - 0s - loss: 0.5629 - val_loss: 0.5654\n",
      "Epoch 185/300\n",
      "23/23 - 0s - loss: 0.5628 - val_loss: 0.5653\n",
      "Epoch 186/300\n",
      "23/23 - 0s - loss: 0.5626 - val_loss: 0.5652\n",
      "Epoch 187/300\n",
      "23/23 - 0s - loss: 0.5624 - val_loss: 0.5651\n",
      "Epoch 188/300\n",
      "23/23 - 0s - loss: 0.5623 - val_loss: 0.5651\n",
      "Epoch 189/300\n",
      "23/23 - 0s - loss: 0.5621 - val_loss: 0.5650\n",
      "Epoch 190/300\n",
      "23/23 - 0s - loss: 0.5620 - val_loss: 0.5649\n",
      "Epoch 191/300\n",
      "23/23 - 0s - loss: 0.5618 - val_loss: 0.5649\n",
      "Epoch 192/300\n",
      "23/23 - 0s - loss: 0.5617 - val_loss: 0.5648\n",
      "Epoch 193/300\n",
      "23/23 - 0s - loss: 0.5616 - val_loss: 0.5647\n",
      "Epoch 194/300\n",
      "23/23 - 0s - loss: 0.5614 - val_loss: 0.5647\n",
      "Epoch 195/300\n",
      "23/23 - 0s - loss: 0.5613 - val_loss: 0.5646\n",
      "Epoch 196/300\n",
      "23/23 - 0s - loss: 0.5612 - val_loss: 0.5646\n",
      "Epoch 197/300\n",
      "23/23 - 0s - loss: 0.5610 - val_loss: 0.5645\n",
      "Epoch 198/300\n",
      "23/23 - 0s - loss: 0.5609 - val_loss: 0.5645\n",
      "Epoch 199/300\n",
      "23/23 - 0s - loss: 0.5608 - val_loss: 0.5644\n",
      "Epoch 200/300\n",
      "23/23 - 0s - loss: 0.5606 - val_loss: 0.5644\n",
      "Epoch 201/300\n",
      "23/23 - 0s - loss: 0.5605 - val_loss: 0.5643\n",
      "Epoch 202/300\n",
      "23/23 - 0s - loss: 0.5604 - val_loss: 0.5643\n",
      "Epoch 203/300\n",
      "23/23 - 0s - loss: 0.5603 - val_loss: 0.5642\n",
      "Epoch 204/300\n",
      "23/23 - 0s - loss: 0.5601 - val_loss: 0.5642\n",
      "Epoch 205/300\n",
      "23/23 - 0s - loss: 0.5600 - val_loss: 0.5641\n",
      "Epoch 206/300\n",
      "23/23 - 0s - loss: 0.5599 - val_loss: 0.5641\n",
      "Epoch 207/300\n",
      "23/23 - 0s - loss: 0.5598 - val_loss: 0.5641\n",
      "Epoch 208/300\n",
      "23/23 - 0s - loss: 0.5597 - val_loss: 0.5640\n",
      "Epoch 209/300\n",
      "23/23 - 0s - loss: 0.5596 - val_loss: 0.5640\n",
      "Epoch 210/300\n",
      "23/23 - 0s - loss: 0.5595 - val_loss: 0.5640\n",
      "Epoch 211/300\n",
      "23/23 - 0s - loss: 0.5594 - val_loss: 0.5639\n",
      "Epoch 212/300\n",
      "23/23 - 0s - loss: 0.5593 - val_loss: 0.5639\n",
      "Epoch 213/300\n",
      "23/23 - 0s - loss: 0.5592 - val_loss: 0.5639\n",
      "Epoch 214/300\n",
      "23/23 - 0s - loss: 0.5591 - val_loss: 0.5638\n",
      "Epoch 215/300\n",
      "23/23 - 0s - loss: 0.5590 - val_loss: 0.5638\n",
      "Epoch 216/300\n",
      "23/23 - 0s - loss: 0.5589 - val_loss: 0.5638\n",
      "Epoch 217/300\n",
      "23/23 - 0s - loss: 0.5588 - val_loss: 0.5637\n",
      "Epoch 218/300\n",
      "23/23 - 0s - loss: 0.5587 - val_loss: 0.5637\n",
      "Epoch 219/300\n",
      "23/23 - 0s - loss: 0.5586 - val_loss: 0.5637\n",
      "Epoch 220/300\n",
      "23/23 - 0s - loss: 0.5585 - val_loss: 0.5637\n",
      "Epoch 221/300\n",
      "23/23 - 0s - loss: 0.5584 - val_loss: 0.5636\n",
      "Epoch 222/300\n",
      "23/23 - 0s - loss: 0.5583 - val_loss: 0.5636\n",
      "Epoch 223/300\n",
      "23/23 - 0s - loss: 0.5582 - val_loss: 0.5636\n",
      "Epoch 224/300\n",
      "23/23 - 0s - loss: 0.5581 - val_loss: 0.5636\n",
      "Epoch 225/300\n",
      "23/23 - 0s - loss: 0.5580 - val_loss: 0.5635\n",
      "Epoch 226/300\n",
      "23/23 - 0s - loss: 0.5580 - val_loss: 0.5635\n",
      "Epoch 227/300\n",
      "23/23 - 0s - loss: 0.5579 - val_loss: 0.5635\n",
      "Epoch 228/300\n",
      "23/23 - 0s - loss: 0.5578 - val_loss: 0.5635\n",
      "Epoch 229/300\n",
      "23/23 - 0s - loss: 0.5577 - val_loss: 0.5635\n",
      "Epoch 230/300\n",
      "23/23 - 0s - loss: 0.5576 - val_loss: 0.5634\n",
      "Epoch 231/300\n",
      "23/23 - 0s - loss: 0.5576 - val_loss: 0.5634\n",
      "Epoch 232/300\n",
      "23/23 - 0s - loss: 0.5575 - val_loss: 0.5634\n",
      "Epoch 233/300\n",
      "23/23 - 0s - loss: 0.5574 - val_loss: 0.5634\n",
      "Epoch 234/300\n",
      "23/23 - 0s - loss: 0.5573 - val_loss: 0.5634\n",
      "Epoch 235/300\n",
      "23/23 - 0s - loss: 0.5572 - val_loss: 0.5634\n",
      "Epoch 236/300\n",
      "23/23 - 0s - loss: 0.5572 - val_loss: 0.5633\n",
      "Epoch 237/300\n",
      "23/23 - 0s - loss: 0.5571 - val_loss: 0.5633\n",
      "Epoch 238/300\n",
      "23/23 - 0s - loss: 0.5570 - val_loss: 0.5633\n",
      "Epoch 239/300\n",
      "23/23 - 0s - loss: 0.5570 - val_loss: 0.5633\n",
      "Epoch 240/300\n",
      "23/23 - 0s - loss: 0.5569 - val_loss: 0.5633\n",
      "Epoch 241/300\n",
      "23/23 - 0s - loss: 0.5568 - val_loss: 0.5633\n",
      "Epoch 242/300\n",
      "23/23 - 0s - loss: 0.5568 - val_loss: 0.5633\n",
      "Epoch 243/300\n",
      "23/23 - 0s - loss: 0.5567 - val_loss: 0.5633\n",
      "Epoch 244/300\n",
      "23/23 - 0s - loss: 0.5566 - val_loss: 0.5632\n",
      "Epoch 245/300\n",
      "23/23 - 0s - loss: 0.5566 - val_loss: 0.5632\n",
      "Epoch 246/300\n",
      "23/23 - 0s - loss: 0.5565 - val_loss: 0.5632\n",
      "Epoch 247/300\n",
      "23/23 - 0s - loss: 0.5564 - val_loss: 0.5632\n",
      "Epoch 248/300\n",
      "23/23 - 0s - loss: 0.5564 - val_loss: 0.5632\n",
      "Epoch 249/300\n",
      "23/23 - 0s - loss: 0.5563 - val_loss: 0.5632\n",
      "Epoch 250/300\n",
      "23/23 - 0s - loss: 0.5562 - val_loss: 0.5632\n",
      "Epoch 251/300\n",
      "23/23 - 0s - loss: 0.5562 - val_loss: 0.5632\n",
      "Epoch 252/300\n",
      "23/23 - 0s - loss: 0.5561 - val_loss: 0.5632\n",
      "Epoch 253/300\n",
      "23/23 - 0s - loss: 0.5561 - val_loss: 0.5631\n",
      "Epoch 254/300\n",
      "23/23 - 0s - loss: 0.5560 - val_loss: 0.5631\n",
      "Epoch 255/300\n",
      "23/23 - 0s - loss: 0.5559 - val_loss: 0.5631\n",
      "Epoch 256/300\n",
      "23/23 - 0s - loss: 0.5559 - val_loss: 0.5631\n",
      "Epoch 257/300\n",
      "23/23 - 0s - loss: 0.5558 - val_loss: 0.5631\n",
      "Epoch 258/300\n",
      "23/23 - 0s - loss: 0.5558 - val_loss: 0.5631\n",
      "Epoch 259/300\n",
      "23/23 - 0s - loss: 0.5557 - val_loss: 0.5631\n",
      "Epoch 260/300\n",
      "23/23 - 0s - loss: 0.5556 - val_loss: 0.5631\n",
      "Epoch 261/300\n",
      "23/23 - 0s - loss: 0.5556 - val_loss: 0.5631\n",
      "Epoch 262/300\n",
      "23/23 - 0s - loss: 0.5555 - val_loss: 0.5631\n",
      "Epoch 263/300\n",
      "23/23 - 0s - loss: 0.5555 - val_loss: 0.5631\n",
      "Epoch 264/300\n",
      "23/23 - 0s - loss: 0.5554 - val_loss: 0.5630\n",
      "Epoch 265/300\n",
      "23/23 - 0s - loss: 0.5554 - val_loss: 0.5630\n",
      "Epoch 266/300\n",
      "23/23 - 0s - loss: 0.5553 - val_loss: 0.5630\n",
      "Epoch 267/300\n",
      "23/23 - 0s - loss: 0.5553 - val_loss: 0.5630\n",
      "Epoch 268/300\n",
      "23/23 - 0s - loss: 0.5552 - val_loss: 0.5630\n",
      "Epoch 269/300\n",
      "23/23 - 0s - loss: 0.5552 - val_loss: 0.5630\n",
      "Epoch 270/300\n",
      "23/23 - 0s - loss: 0.5551 - val_loss: 0.5630\n",
      "Epoch 271/300\n",
      "23/23 - 0s - loss: 0.5551 - val_loss: 0.5630\n",
      "Epoch 272/300\n",
      "23/23 - 0s - loss: 0.5550 - val_loss: 0.5630\n",
      "Epoch 273/300\n",
      "23/23 - 0s - loss: 0.5550 - val_loss: 0.5630\n",
      "Epoch 274/300\n",
      "23/23 - 0s - loss: 0.5549 - val_loss: 0.5630\n",
      "Epoch 275/300\n",
      "23/23 - 0s - loss: 0.5549 - val_loss: 0.5629\n",
      "Epoch 276/300\n",
      "23/23 - 0s - loss: 0.5548 - val_loss: 0.5629\n",
      "Epoch 277/300\n",
      "23/23 - 0s - loss: 0.5548 - val_loss: 0.5629\n",
      "Epoch 278/300\n",
      "23/23 - 0s - loss: 0.5547 - val_loss: 0.5629\n",
      "Epoch 279/300\n",
      "23/23 - 0s - loss: 0.5547 - val_loss: 0.5629\n",
      "Epoch 280/300\n",
      "23/23 - 0s - loss: 0.5546 - val_loss: 0.5629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/300\n",
      "23/23 - 0s - loss: 0.5546 - val_loss: 0.5629\n",
      "Epoch 282/300\n",
      "23/23 - 0s - loss: 0.5545 - val_loss: 0.5629\n",
      "Epoch 283/300\n",
      "23/23 - 0s - loss: 0.5545 - val_loss: 0.5629\n",
      "Epoch 284/300\n",
      "23/23 - 0s - loss: 0.5545 - val_loss: 0.5629\n",
      "Epoch 285/300\n",
      "23/23 - 0s - loss: 0.5544 - val_loss: 0.5629\n",
      "Epoch 286/300\n",
      "23/23 - 0s - loss: 0.5544 - val_loss: 0.5629\n",
      "Epoch 287/300\n",
      "23/23 - 0s - loss: 0.5543 - val_loss: 0.5629\n",
      "Epoch 288/300\n",
      "23/23 - 0s - loss: 0.5543 - val_loss: 0.5628\n",
      "Epoch 289/300\n",
      "23/23 - 0s - loss: 0.5542 - val_loss: 0.5628\n",
      "Epoch 290/300\n",
      "23/23 - 0s - loss: 0.5542 - val_loss: 0.5628\n",
      "Epoch 291/300\n",
      "23/23 - 0s - loss: 0.5542 - val_loss: 0.5628\n",
      "Epoch 292/300\n",
      "23/23 - 0s - loss: 0.5541 - val_loss: 0.5628\n",
      "Epoch 293/300\n",
      "23/23 - 0s - loss: 0.5541 - val_loss: 0.5628\n",
      "Epoch 294/300\n",
      "23/23 - 0s - loss: 0.5540 - val_loss: 0.5628\n",
      "Epoch 295/300\n",
      "23/23 - 0s - loss: 0.5540 - val_loss: 0.5628\n",
      "Epoch 296/300\n",
      "23/23 - 0s - loss: 0.5539 - val_loss: 0.5628\n",
      "Epoch 297/300\n",
      "23/23 - 0s - loss: 0.5539 - val_loss: 0.5628\n",
      "Epoch 298/300\n",
      "23/23 - 0s - loss: 0.5539 - val_loss: 0.5628\n",
      "Epoch 299/300\n",
      "23/23 - 0s - loss: 0.5538 - val_loss: 0.5627\n",
      "Epoch 300/300\n",
      "23/23 - 0s - loss: 0.5538 - val_loss: 0.5627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe910141d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's train the autoencoder, checking the progress on a validation dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X2, Y1, Y2 = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "Y1 = np.asarray(Y1).astype('float32').reshape((-1,1))\n",
    "Y2 = np.asarray(Y2).astype('float32').reshape((-1,1))\n",
    "\n",
    "\n",
    "autoencoder.fit(X1, Y1,\n",
    "                epochs=300,\n",
    "                batch_size=200,\n",
    "                shuffle=False,\n",
    "                verbose = 2,\n",
    "                validation_data=(X2, Y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the coding of the initial features\n",
    "encoder = Model(input_layer, encoded)\n",
    "ae = encoder.predict(X_train)\n",
    "ae_test = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-prescription",
   "metadata": {},
   "source": [
    "## TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "parliamentary-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X_train)\n",
    "X_embedded_test = TSNE(n_components=2).fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-differential",
   "metadata": {},
   "source": [
    "# Elbow method to choose clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "built-hazard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 10)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    #model.fit(pc) #PCA  --> 5\n",
    "    #model.fit(ic) # ICA  --> 5\n",
    "    #model.fit(ae) # Autoencoder --> 3\n",
    "    model.fit(X_embedded) # TSNE  --> 4\n",
    "    \n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "complex-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ICA and PCA (first 2 components)\n",
    "from sklearn.decomposition import PCA, FastICA # Principal Component Analysis module\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "def get_trained_encoder(X_train, red_dim = 10):\n",
    "    '''\n",
    "    Returns the trained encoder part of a autoencoder\n",
    "    '''\n",
    "    encoding_dim = red_dim\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(X_train.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "    # let's create and compile the autoencoder\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    X1, X2, Y1, Y2 = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    Y1 = np.asarray(Y1).astype('float32').reshape((-1,1))\n",
    "    Y2 = np.asarray(Y2).astype('float32').reshape((-1,1))\n",
    "\n",
    "\n",
    "    autoencoder.fit(X1, Y1,\n",
    "                    epochs=300,\n",
    "                    batch_size=200,\n",
    "                    shuffle=False,\n",
    "                    verbose = 0,\n",
    "                    validation_data=(X2, Y2))\n",
    "    \n",
    "    \n",
    "    encoder = Model(input_layer, encoded)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def classify_clusters(X_train, y_train, X_test, y_test, clf, \n",
    "                                   dim_red_methods = {'ica': 5,\n",
    "                                                      'pca': 5, 'autoencoder' : 3,\n",
    "                                                      'tsne': 4}, leave_one_out = False):\n",
    "    \n",
    "    clustering_approaches = {} # Dictionary storing the dimentionality reduced version of the dataset \n",
    "    ## ICA \n",
    "    if 'ica' in dim_red_methods.keys():\n",
    "        print('Performing ICA dimentionality reduction')\n",
    "        ica = FastICA(n_components=dim_red_methods['ica'])\n",
    "        ica.fit(X_train)\n",
    "        clustering_approaches['ica'] = {\n",
    "            'train': ica.transform(X_train),\n",
    "            'test': ica.transform(X_test),\n",
    "            'k' : 5\n",
    "        }\n",
    "    \n",
    "    ## PCA \n",
    "    if 'pca' in dim_red_methods.keys():\n",
    "        print('Performing PCA dimentionality reduction')\n",
    "        pca = PCA(n_components=dim_red_methods['pca'])\n",
    "        pca.fit(X_train)\n",
    "        clustering_approaches['pca'] = {\n",
    "            'train': pca.transform(X_train),\n",
    "            'test': pca.transform(X_test),\n",
    "            'k' : 5\n",
    "        }\n",
    "    ## Autoencoder\n",
    "    if 'autoencoder' in dim_red_methods.keys():\n",
    "        print('Performing autoencoder dimentionality reduction, by training autoencoder, and returning encoder')\n",
    "        encoder = get_trained_encoder(X_train, dim_red_methods['autoencoder'])\n",
    "        clustering_approaches['autoencoder'] = {\n",
    "            'train': encoder.predict(X_train),\n",
    "            'test': encoder.predict(X_test),\n",
    "            'k' : 3\n",
    "        }\n",
    "    ## TSNE   \n",
    "    if 'tsne' in dim_red_methods.keys():\n",
    "        print('Performing TSNE dimentionality reduction')\n",
    "        clustering_approaches['tsne'] = {\n",
    "            'train': TSNE(n_components= dim_red_methods['tsne']).fit_transform(X_train),\n",
    "            'test': TSNE(n_components= dim_red_methods['tsne']).fit_transform(X_test),\n",
    "            'k' : 4\n",
    "        }\n",
    "    print('Dimentionality reduction dictionary is found, now training initial classifier')\n",
    "    score_init = {}\n",
    "    clf_init = clf.fit(X_train, y_train)\n",
    "    y_pred_init = clf_init.predict(X_test)\n",
    "    score_init['f1-score'] = f1_score(y_test, y_pred_init)\n",
    "    score_init['accuracy'] = accuracy_score(y_test, y_pred_init)\n",
    "\n",
    "    def clustering_based_filter(dim_red_train, dim_red_test, k):\n",
    "        '''\n",
    "        Uses the defined dimentionality reduced version of the dataset, with k means clustering, \n",
    "        to form clusters in the test set, which are tested seperatly. By either leaving one cluster out, \n",
    "        or only including one cluster\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        model = KMeans(n_clusters=k)\n",
    "        model.fit(dim_red_train)\n",
    "\n",
    "        new_y_test = model.predict(dim_red_test)\n",
    "        cluster_dict = {}\n",
    "        for cluster in range(k): \n",
    "            if leave_one_out:\n",
    "                cluster_dict[cluster] = np.where(new_y_test != cluster)[0]  # Leave one cluster, test on rest\n",
    "            else:\n",
    "                cluster_dict[cluster] = np.where(new_y_test == cluster)[0]  # Only test on the items in the cluster\n",
    "        metrics = ['accuracy', 'f1-score']\n",
    "        scores = {k: [] for k in metrics}\n",
    "\n",
    "        percentage_of_del_idx = []\n",
    "        for cluster_nr, indices in cluster_dict.items():\n",
    "            X_test_temp = X_test.iloc[indices]\n",
    "            y_test_temp = y_test[indices]\n",
    "            percentage_of_del_idx.append(1 - X_test_temp.shape[0]/X_test.shape[0])\n",
    "            y_pred = clf_init.predict(X_test_temp)\n",
    "            scores['accuracy'].append(accuracy_score(y_test_temp, y_pred))\n",
    "            scores['f1-score'].append(f1_score(y_test_temp, y_pred))\n",
    "\n",
    "        return scores, percentage_of_del_idx, cluster_dict\n",
    "    \n",
    "    scores, per, clusters = {}, {}, {}\n",
    "    for dim_red, args in clustering_approaches.items():\n",
    "        print(f'Clustering {dim_red}, and predicting on clusters')\n",
    "        scores[dim_red], per[dim_red], clusters[dim_red] = clustering_based_filter(args['train'],\n",
    "                                                                                   args['test'],\n",
    "                                                                                   args['k'])\n",
    "    def get_best_performing_cluster_into():\n",
    "        '''\n",
    "        Returns a dictionary belonging to each dimentionality reduction technique\n",
    "        \n",
    "        '''\n",
    "        new_scores = {}\n",
    "        for dim_red, met_dict in scores.items():\n",
    "            saved_info = {}\n",
    "            acc_vals = np.array(met_dict['accuracy'])\n",
    "            f1_vals = np.array(met_dict['f1-score'])\n",
    "            max_combined_idx = np.argmax(acc_vals + f1_vals)\n",
    "            saved_info = {\n",
    "                'best accuracy': acc_vals[max_combined_idx],\n",
    "                'best f1-score': f1_vals[max_combined_idx],\n",
    "                'cluster number' : max_combined_idx, \n",
    "                'included indices': clusters[dim_red][max_combined_idx],\n",
    "                'percentage of data removed' : per[dim_red][max_combined_idx]\n",
    "\n",
    "            }\n",
    "            new_scores[dim_red] = saved_info\n",
    "        return new_scores\n",
    "    \n",
    "    return score_init, get_best_performing_cluster_into()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "seven-destiny",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimentionality reduction dictionary is found, now training initial classifier\n",
      "Clustering ica, and predicting on clusters\n",
      "Clustering pca, and predicting on clusters\n",
      "Clustering autoencoder, and predicting on clusters\n",
      "Clustering tsne, and predicting on clusters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(leaf_size=1, n_neighbors=1, p=1)\n",
    "best_clusters = classify_clusters(X_train, y_train, X_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "returning-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ica': {'best accuracy': 0.7665289256198347,\n",
       "  'best f1-score': 0.7414187643020596,\n",
       "  'cluster number': 4,\n",
       "  'included indices': array([   1,    2,    8,   12,   13,   14,   17,   19,   27,   29,   31,\n",
       "           32,   33,   35,   36,   37,   45,   46,   50,   53,   55,   60,\n",
       "           61,   62,   63,   67,   68,   71,   75,   77,   78,   85,   88,\n",
       "           89,   94,   96,   97,   99,  105,  109,  111,  112,  115,  116,\n",
       "          119,  123,  124,  126,  127,  131,  132,  134,  135,  143,  145,\n",
       "          147,  149,  150,  153,  158,  159,  166,  171,  172,  175,  176,\n",
       "          177,  178,  182,  186,  192,  194,  195,  199,  200,  202,  206,\n",
       "          207,  209,  210,  212,  217,  221,  222,  225,  229,  231,  233,\n",
       "          234,  235,  236,  237,  238,  239,  244,  251,  253,  260,  264,\n",
       "          267,  268,  272,  273,  275,  276,  279,  280,  284,  287,  288,\n",
       "          290,  296,  298,  304,  305,  307,  308,  310,  316,  325,  329,\n",
       "          338,  340,  341,  344,  351,  355,  359,  360,  361,  362,  367,\n",
       "          368,  374,  376,  377,  378,  382,  383,  388,  390,  396,  397,\n",
       "          402,  403,  407,  410,  411,  412,  416,  418,  423,  424,  429,\n",
       "          430,  431,  432,  433,  437,  438,  439,  440,  443,  445,  446,\n",
       "          461,  462,  463,  464,  467,  471,  472,  474,  475,  476,  478,\n",
       "          479,  483,  487,  494,  495,  497,  500,  501,  503,  505,  506,\n",
       "          508,  509,  523,  528,  529,  533,  534,  540,  541,  543,  546,\n",
       "          547,  550,  554,  557,  559,  563,  564,  571,  573,  574,  576,\n",
       "          580,  582,  584,  585,  590,  592,  595,  597,  602,  603,  604,\n",
       "          606,  607,  611,  612,  613,  615,  623,  632,  633,  635,  638,\n",
       "          639,  640,  642,  645,  649,  650,  652,  657,  658,  663,  665,\n",
       "          667,  674,  676,  680,  681,  682,  683,  686,  688,  692,  695,\n",
       "          698,  710,  711,  713,  720,  721,  723,  724,  725,  726,  727,\n",
       "          728,  731,  734,  745,  755,  759,  764,  769,  770,  772,  773,\n",
       "          774,  782,  784,  785,  786,  787,  791,  793,  797,  804,  807,\n",
       "          809,  810,  811,  816,  829,  833,  834,  838,  839,  840,  842,\n",
       "          843,  846,  849,  851,  853,  854,  855,  856,  861,  867,  871,\n",
       "          873,  876,  881,  884,  887,  888,  891,  895,  897,  899,  901,\n",
       "          907,  908,  913,  914,  915,  923,  926,  929,  936,  941,  943,\n",
       "          949,  950,  956,  963,  966,  967,  968,  970,  975,  977,  978,\n",
       "          983,  987,  988,  991,  995,  998, 1000, 1002, 1006, 1007, 1008,\n",
       "         1011, 1012, 1015, 1017, 1018, 1019, 1030, 1036, 1037, 1039, 1041,\n",
       "         1042, 1044, 1046, 1056, 1067, 1073, 1076, 1078, 1079, 1081, 1086,\n",
       "         1087, 1088, 1089, 1090, 1093, 1095, 1101, 1106, 1109, 1113, 1119,\n",
       "         1120, 1124, 1128, 1136, 1138, 1140, 1141, 1143, 1144, 1145, 1146,\n",
       "         1148, 1150, 1151, 1152, 1154, 1157, 1159, 1160, 1162, 1165, 1166,\n",
       "         1170, 1173, 1174, 1192, 1193, 1194, 1196, 1197, 1201, 1202, 1203,\n",
       "         1206, 1207, 1211, 1212, 1213, 1217, 1223, 1224, 1229, 1231, 1232,\n",
       "         1239, 1240, 1241, 1242, 1248, 1250, 1251, 1253, 1256, 1260, 1261,\n",
       "         1269, 1272, 1274, 1275, 1276, 1277, 1278, 1284, 1286, 1288, 1289,\n",
       "         1290, 1292, 1293, 1297, 1299, 1301, 1307, 1308, 1313, 1315, 1316,\n",
       "         1318, 1323, 1329, 1331, 1334, 1336, 1337, 1339, 1342, 1348, 1352,\n",
       "         1353, 1354, 1356, 1359, 1363, 1366, 1367, 1368, 1370, 1375, 1377]),\n",
       "  'percentage of data removed': 0.6492753623188405},\n",
       " 'pca': {'best accuracy': 0.7708779443254818,\n",
       "  'best f1-score': 0.7458432304038005,\n",
       "  'cluster number': 4,\n",
       "  'included indices': array([   1,    2,    8,   12,   13,   14,   17,   19,   27,   29,   31,\n",
       "           32,   33,   35,   36,   37,   45,   46,   50,   53,   55,   61,\n",
       "           62,   63,   67,   68,   75,   77,   85,   88,   89,   94,   96,\n",
       "           97,   99,  105,  109,  111,  112,  115,  116,  119,  123,  124,\n",
       "          126,  127,  131,  132,  134,  135,  143,  145,  147,  149,  153,\n",
       "          158,  159,  166,  171,  172,  176,  177,  178,  182,  186,  192,\n",
       "          194,  195,  199,  200,  202,  206,  207,  209,  210,  212,  217,\n",
       "          221,  222,  225,  229,  231,  233,  234,  235,  236,  237,  238,\n",
       "          239,  244,  251,  253,  260,  264,  268,  272,  273,  275,  276,\n",
       "          279,  280,  287,  288,  290,  296,  298,  304,  305,  307,  308,\n",
       "          310,  316,  325,  329,  338,  340,  341,  344,  351,  355,  359,\n",
       "          360,  361,  362,  367,  368,  374,  376,  377,  378,  382,  383,\n",
       "          388,  390,  397,  402,  403,  407,  410,  411,  412,  416,  418,\n",
       "          423,  424,  429,  430,  431,  432,  433,  437,  438,  439,  440,\n",
       "          443,  445,  446,  461,  462,  463,  464,  467,  471,  472,  474,\n",
       "          476,  478,  479,  483,  487,  494,  495,  497,  500,  501,  503,\n",
       "          505,  506,  508,  509,  523,  528,  529,  533,  534,  540,  541,\n",
       "          543,  546,  547,  550,  554,  557,  559,  563,  564,  571,  573,\n",
       "          574,  576,  580,  582,  585,  590,  592,  595,  597,  602,  603,\n",
       "          604,  606,  607,  611,  612,  613,  615,  623,  632,  633,  635,\n",
       "          638,  639,  640,  642,  645,  649,  650,  652,  657,  658,  663,\n",
       "          665,  667,  674,  676,  680,  681,  682,  683,  688,  692,  695,\n",
       "          698,  710,  711,  713,  720,  721,  723,  724,  725,  726,  727,\n",
       "          728,  731,  734,  745,  755,  759,  764,  769,  770,  772,  773,\n",
       "          782,  784,  785,  786,  787,  791,  793,  797,  804,  807,  809,\n",
       "          810,  811,  816,  829,  833,  834,  838,  839,  840,  842,  843,\n",
       "          846,  849,  851,  853,  854,  855,  856,  861,  867,  871,  873,\n",
       "          876,  881,  884,  887,  888,  891,  895,  897,  899,  901,  907,\n",
       "          908,  913,  914,  915,  923,  926,  929,  936,  941,  943,  949,\n",
       "          950,  956,  963,  966,  967,  968,  970,  975,  977,  978,  983,\n",
       "          987,  988,  991,  995,  998, 1000, 1002, 1006, 1007, 1008, 1011,\n",
       "         1012, 1015, 1017, 1018, 1019, 1030, 1036, 1037, 1039, 1041, 1042,\n",
       "         1044, 1046, 1056, 1067, 1073, 1076, 1078, 1079, 1081, 1086, 1087,\n",
       "         1088, 1089, 1090, 1093, 1095, 1101, 1106, 1109, 1119, 1120, 1124,\n",
       "         1128, 1136, 1138, 1140, 1141, 1143, 1144, 1145, 1146, 1148, 1150,\n",
       "         1151, 1152, 1154, 1157, 1159, 1160, 1162, 1165, 1166, 1170, 1173,\n",
       "         1174, 1193, 1194, 1196, 1197, 1201, 1202, 1203, 1206, 1207, 1211,\n",
       "         1212, 1213, 1217, 1223, 1224, 1229, 1231, 1232, 1239, 1240, 1241,\n",
       "         1242, 1248, 1250, 1251, 1253, 1256, 1260, 1261, 1269, 1272, 1275,\n",
       "         1276, 1277, 1278, 1284, 1288, 1289, 1290, 1292, 1293, 1297, 1299,\n",
       "         1301, 1307, 1308, 1313, 1315, 1316, 1318, 1323, 1329, 1331, 1334,\n",
       "         1336, 1337, 1339, 1342, 1348, 1352, 1353, 1354, 1356, 1363, 1366,\n",
       "         1367, 1368, 1370, 1375, 1377]),\n",
       "  'percentage of data removed': 0.6615942028985506},\n",
       " 'autoencoder': {'best accuracy': 0.8150807899461401,\n",
       "  'best f1-score': 0.6906906906906908,\n",
       "  'cluster number': 0,\n",
       "  'included indices': array([   3,    6,    9,   13,   15,   21,   23,   26,   27,   28,   29,\n",
       "           30,   31,   33,   38,   40,   41,   42,   44,   56,   57,   58,\n",
       "           59,   62,   65,   66,   69,   70,   74,   75,   76,   79,   81,\n",
       "           84,   90,   92,   93,   95,   98,  100,  104,  109,  110,  113,\n",
       "          118,  119,  121,  129,  130,  136,  139,  140,  144,  148,  152,\n",
       "          155,  156,  157,  160,  162,  163,  164,  168,  173,  174,  175,\n",
       "          180,  184,  185,  187,  188,  189,  194,  197,  198,  203,  205,\n",
       "          208,  209,  211,  214,  215,  217,  218,  221,  224,  226,  227,\n",
       "          228,  230,  231,  232,  236,  240,  241,  245,  247,  250,  254,\n",
       "          255,  259,  263,  266,  274,  277,  283,  285,  286,  287,  292,\n",
       "          293,  295,  297,  299,  301,  311,  312,  314,  318,  319,  321,\n",
       "          322,  327,  332,  345,  346,  347,  348,  349,  350,  356,  357,\n",
       "          358,  362,  363,  365,  369,  370,  375,  377,  379,  381,  384,\n",
       "          385,  388,  390,  391,  392,  395,  397,  399,  400,  401,  405,\n",
       "          406,  419,  421,  422,  431,  434,  435,  436,  441,  444,  447,\n",
       "          448,  449,  450,  451,  452,  455,  456,  458,  459,  460,  461,\n",
       "          469,  470,  471,  472,  476,  477,  480,  481,  484,  485,  486,\n",
       "          487,  488,  489,  491,  498,  499,  501,  502,  510,  511,  512,\n",
       "          513,  519,  520,  521,  522,  525,  526,  527,  530,  536,  542,\n",
       "          544,  545,  552,  553,  559,  561,  562,  563,  567,  568,  570,\n",
       "          572,  573,  579,  581,  584,  589,  591,  593,  594,  596,  599,\n",
       "          600,  603,  614,  615,  616,  617,  618,  619,  621,  622,  625,\n",
       "          626,  628,  630,  631,  632,  634,  636,  637,  643,  644,  646,\n",
       "          648,  651,  655,  657,  660,  661,  662,  668,  670,  671,  673,\n",
       "          676,  677,  678,  679,  682,  685,  686,  690,  694,  697,  699,\n",
       "          700,  702,  703,  704,  705,  706,  707,  708,  712,  714,  716,\n",
       "          717,  719,  724,  729,  730,  731,  732,  733,  735,  736,  737,\n",
       "          738,  739,  740,  741,  742,  746,  747,  748,  750,  752,  755,\n",
       "          756,  763,  765,  766,  768,  774,  776,  778,  779,  780,  781,\n",
       "          782,  783,  786,  788,  791,  794,  795,  796,  797,  801,  802,\n",
       "          803,  806,  810,  812,  816,  818,  819,  822,  823,  825,  826,\n",
       "          827,  831,  832,  840,  841,  842,  846,  847,  848,  850,  851,\n",
       "          854,  856,  858,  859,  860,  863,  864,  866,  868,  869,  872,\n",
       "          876,  877,  879,  880,  882,  885,  889,  892,  893,  894,  900,\n",
       "          903,  905,  912,  916,  917,  918,  920,  921,  922,  924,  925,\n",
       "          930,  933,  935,  940,  944,  945,  947,  948,  951,  953,  961,\n",
       "          962,  969,  974,  976,  979,  982,  984,  986,  990,  992,  993,\n",
       "          994,  996,  997,  998, 1001, 1003, 1004, 1005, 1014, 1021, 1027,\n",
       "         1031, 1032, 1033, 1035, 1040, 1044, 1045, 1046, 1047, 1048, 1049,\n",
       "         1050, 1051, 1054, 1057, 1059, 1060, 1061, 1062, 1063, 1064, 1071,\n",
       "         1072, 1080, 1081, 1082, 1084, 1085, 1094, 1096, 1103, 1105, 1112,\n",
       "         1116, 1117, 1118, 1121, 1123, 1124, 1127, 1130, 1132, 1137, 1139,\n",
       "         1141, 1147, 1148, 1149, 1153, 1155, 1156, 1161, 1163, 1166, 1168,\n",
       "         1169, 1175, 1177, 1182, 1183, 1184, 1185, 1187, 1188, 1190, 1191,\n",
       "         1193, 1196, 1198, 1200, 1202, 1205, 1206, 1207, 1208, 1209, 1210,\n",
       "         1212, 1213, 1215, 1216, 1218, 1219, 1220, 1222, 1226, 1227, 1230,\n",
       "         1236, 1237, 1238, 1245, 1251, 1253, 1254, 1256, 1259, 1260, 1262,\n",
       "         1263, 1267, 1273, 1274, 1276, 1278, 1279, 1283, 1287, 1289, 1291,\n",
       "         1293, 1295, 1296, 1300, 1302, 1304, 1309, 1311, 1312, 1320, 1321,\n",
       "         1324, 1331, 1332, 1336, 1340, 1341, 1343, 1346, 1349, 1357, 1361,\n",
       "         1371, 1372, 1373, 1374, 1377, 1378, 1379]),\n",
       "  'percentage of data removed': 0.596376811594203},\n",
       " 'tsne': {'best accuracy': 0.8064516129032258,\n",
       "  'best f1-score': 0.7090909090909091,\n",
       "  'cluster number': 1,\n",
       "  'included indices': array([   0,    1,    2,    3,    5,    7,    9,   10,   14,   18,   20,\n",
       "           27,   28,   29,   32,   34,   35,   38,   39,   41,   42,   49,\n",
       "           50,   51,   55,   56,   57,   62,   63,   68,   69,   77,   81,\n",
       "           83,   84,   85,   89,   93,   94,   97,   99,  100,  101,  105,\n",
       "          107,  109,  111,  112,  118,  123,  125,  126,  128,  129,  136,\n",
       "          141,  142,  143,  146,  147,  148,  149,  151,  153,  154,  155,\n",
       "          156,  159,  166,  167,  169,  172,  173,  175,  176,  177,  178,\n",
       "          182,  185,  187,  188,  190,  191,  192,  193,  194,  199,  205,\n",
       "          206,  207,  214,  218,  222,  225,  228,  229,  234,  235,  238,\n",
       "          241,  242,  251,  252,  254,  256,  258,  260,  261,  262,  263,\n",
       "          266,  271,  272,  279,  281,  286,  290,  297,  302,  315,  316,\n",
       "          317,  324,  325,  326,  330,  331,  334,  335,  337,  338,  339,\n",
       "          341,  343,  346,  348,  352,  354,  360,  361,  362,  363,  366,\n",
       "          367,  368,  372,  374,  375,  378,  379,  382,  388,  391,  396,\n",
       "          397,  400,  402,  403,  404,  406,  407,  419,  422,  423,  426,\n",
       "          431,  432,  433,  436,  440,  441,  442,  446,  449,  451,  452,\n",
       "          453,  459,  462,  469,  471,  472,  474,  475,  476,  483,  491,\n",
       "          492,  495,  503,  504,  506,  508,  511,  520,  524,  528,  530,\n",
       "          534,  535,  537,  541,  547,  549,  556,  558,  559,  561,  562,\n",
       "          563,  565,  569,  571,  574,  576,  580,  584,  585,  589,  592,\n",
       "          594,  596,  599,  602,  604,  608,  609,  613,  615,  624,  625,\n",
       "          631,  632,  638,  643,  646,  651,  652,  657,  664,  669,  670,\n",
       "          671,  678,  680,  682,  686,  689,  692,  694,  696,  697,  698,\n",
       "          701,  703,  715,  717,  721,  722,  724,  727,  730,  731,  732,\n",
       "          736,  739,  740,  750,  753,  754,  757,  762,  764,  765,  769,\n",
       "          770,  772,  773,  774,  776,  779,  788,  790,  791,  792,  793,\n",
       "          798,  801,  803,  804,  807,  810,  811,  812,  813,  816,  824,\n",
       "          827,  828,  833,  836,  837,  840,  842,  846,  851,  853,  854,\n",
       "          862,  868,  871,  872,  874,  876,  879,  880,  886,  887,  888,\n",
       "          889,  897,  901,  902,  907,  913,  915,  919,  920,  922,  927,\n",
       "          929,  931,  933,  934,  935,  937,  939,  942,  943,  947,  948,\n",
       "          950,  952,  960,  962,  963,  964,  965,  966,  971,  972,  973,\n",
       "          974,  975,  978,  979,  981,  982,  984,  988,  990,  993,  994,\n",
       "          995,  998, 1001, 1002, 1008, 1012, 1014, 1015, 1016, 1019, 1025,\n",
       "         1026, 1034, 1036, 1046, 1049, 1050, 1056, 1057, 1058, 1063, 1064,\n",
       "         1065, 1066, 1067, 1072, 1074, 1078, 1080, 1081, 1087, 1090, 1092,\n",
       "         1098, 1103, 1110, 1119, 1120, 1121, 1123, 1124, 1126, 1131, 1134,\n",
       "         1135, 1137, 1145, 1148, 1150, 1156, 1163, 1166, 1170, 1172, 1173,\n",
       "         1174, 1183, 1184, 1188, 1189, 1193, 1195, 1196, 1197, 1199, 1200,\n",
       "         1201, 1204, 1207, 1209, 1210, 1211, 1212, 1218, 1222, 1223, 1229,\n",
       "         1231, 1234, 1235, 1238, 1239, 1241, 1242, 1243, 1244, 1245, 1246,\n",
       "         1248, 1253, 1255, 1256, 1257, 1258, 1259, 1260, 1264, 1268, 1269,\n",
       "         1271, 1272, 1275, 1276, 1279, 1282, 1283, 1284, 1285, 1288, 1291,\n",
       "         1292, 1295, 1297, 1299, 1306, 1316, 1323, 1327, 1328, 1331, 1341,\n",
       "         1342, 1350, 1353, 1354, 1363, 1365, 1367, 1368, 1371, 1373, 1377,\n",
       "         1379]),\n",
       "  'percentage of data removed': 0.6405797101449275}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dressed-highland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1-score': 0.6960985626283367, 'accuracy': 0.7855072463768116}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_init"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
